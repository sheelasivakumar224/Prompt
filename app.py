from flask import Flask,render_template,stream_template,request,Response
from flask_cors import CORS
import openai
import os
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__)
CORS(app)

openai.api_key = os.environ.get("OPENAI_KEY")

system_gen_system_prompt = """Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.

The prompts you will be generating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.

In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.

You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.

Most importantly, output NOTHING but the prompt. Do not include anything else in your message."""


ranking_system_prompt = """Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.

You will be provided with the task description, the test prompt, and two generations - one for each system prompt.

Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.

Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.

Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.

Respond with your ranking, and nothing else. Be fair and unbiased in your judgement."""

K = 32

CANDIDATE_MODEL = 'gpt-3.5-turbo'
CANDIDATE_MODEL_TEMPERATURE = 0.9
HEADERS = {}

def generate_candidate_prompts(description, test_cases, number_of_prompts):
    outputs = openai.ChatCompletion.create(
        model = CANDIDATE_MODEL,
        messages = [
           {"role": "system", "content": system_gen_system_prompt},
          {"role": "user", "content": f"Here are some test cases:`{test_cases}`\n\nHere is the description of the use-case: `{description.strip()}`\n\nRespond with your prompt, and nothing else. Be creative."}
        ],
        temperature = CANDIDATE_MODEL_TEMPERATURE,
        n = number_of_prompts,
        headers = HEADERS,
        stream = True
    )
    return outputs

@app.route('/')
def index():
    return render_template("index.html")

@app.route('/display',methods=["POST"])
def display():
    if request.method == 'POST':
        messages = request.json['messages']
        test_case = "FrontEnd Developer"
        n = 2
        def event_stream():
            for line in generate_candidate_prompts(description= messages, test_cases=test_case, number_of_prompts=n):
                print(line)
                text = line.choices[0].delta.get('content', '')
                if len(text): 
                    yield text
        return Response(event_stream(),mimetype='text/event-stream')
    else:
        return stream_template('/templates/index.html')

if __name__ == "__main__":
    app.run(debug = True)